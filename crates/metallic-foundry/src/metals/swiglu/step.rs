use std::sync::Once;

use half::f16;
use serde::{Deserialize, Serialize};

// Note: SwigluStep and CompiledSwigluStep are auto-generated by #[derive(Kernel)] in mod.rs
use super::{
    config::{effective_weights_per_block, resolve_rms_eps}, kernels::get_fused_ffn_kernel, runtime::{allocate_zero_bias, run_canonical_projection}
};
use crate::{
    Foundry, MetalError, metals::common::runtime::{require_non_empty_io, require_vector_len, resolve_batch, tail_dim}, spec::{CompiledStep, FastBindings, Ref, ResolvedSymbols, Step, SymbolTable, TensorBindings}, types::{DispatchConfig, MetalResourceOptions, TensorArg}
};

static MIXED_SWIGLU_FALLBACK_WARN_ONCE: Once = Once::new();

impl CompiledStep for super::CompiledSwigluStep {
    fn execute(
        &self,
        foundry: &mut Foundry,
        fast_bindings: &FastBindings,
        bindings: &TensorBindings,
        _symbols: &SymbolTable,
    ) -> Result<(), MetalError> {
        let gate = fast_bindings.get(self.gate).ok_or(MetalError::InputNotFound("gate".into()))?;
        let up = fast_bindings.get(self.up_inout).ok_or(MetalError::InputNotFound("up".into()))?;
        let gate_bias = fast_bindings
            .get(self.gate_bias)
            .ok_or(MetalError::InputNotFound("gate_bias".into()))?;
        let up_bias = fast_bindings.get(self.up_bias).ok_or(MetalError::InputNotFound("up_bias".into()))?;

        let total_elements = up.dims.iter().product::<usize>() as u32;
        let bias_len = self.params.bias_len.resolve(bindings);
        let gate_stride = self.params.gate_leading_stride.resolve(bindings);
        let up_stride = self.params.up_leading_stride.resolve(bindings);

        let kernel = super::Swiglu::new_auto_vectorized(gate, up, gate_bias, up_bias, total_elements, bias_len, gate_stride, up_stride);
        foundry.run(&kernel)
    }

    fn name(&self) -> &'static str {
        "Swiglu"
    }
}

// =============================================================================
// FusedSwigluStep - RMSNorm → Gate/Up GEMVs → SwiGLU activation
// Maps to legacy "SwiGluF16CanonicalFusedRmsnorm" op
// =============================================================================

/// Fused SwiGLU Step: RMSNorm(Input) → Gate/Up GEMVs → SwiGLU
///
/// This maps to the legacy "SwiGluF16CanonicalFusedRmsnorm" op in model specs.
/// Currently implemented as a placeholder that decomposes into separate steps.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FusedSwigluStep {
    pub input: Ref,
    pub gamma: Ref,      // RMSNorm weights
    pub wg: Ref,         // Gate weight matrix (alias for w_gate)
    pub wu: Ref,         // Up weight matrix (alias for w_up)
    pub bg: Option<Ref>, // Gate bias
    pub bu: Option<Ref>, // Up bias
    pub out: Ref,        // Output
    #[serde(default = "default_epsilon")]
    pub epsilon: f32,
    #[serde(default = "default_wpb")]
    pub weights_per_block: u32,
}

fn default_epsilon() -> f32 {
    1e-6
}
fn default_wpb() -> u32 {
    32
}

#[derive(Debug, Clone)]
pub struct CompiledFusedSwigluStep {
    pub step: FusedSwigluStep,
    pub input_idx: usize,
    pub gamma_idx: usize,
    pub wg_name: String,
    pub wg_resolved: ResolvedSymbols,
    pub wu_name: String,
    pub wu_resolved: ResolvedSymbols,
    pub bg_idx: Option<usize>,
    pub bu_idx: Option<usize>,
    pub out_idx: usize,
}

#[typetag::serde(name = "SwiGluF16CanonicalFusedRmsnorm")]
impl Step for FusedSwigluStep {
    fn name(&self) -> &'static str {
        "SwiGluF16CanonicalFusedRmsnorm"
    }

    fn execute(&self, foundry: &mut Foundry, bindings: &mut TensorBindings) -> Result<(), MetalError> {
        let mut symbols = SymbolTable::new();
        let compiled = self.compile(bindings, &mut symbols);
        let mut fast_bindings = FastBindings::new(symbols.len());

        for (name, symbol_id) in symbols.iter() {
            if let Ok(tensor) = bindings.get(name) {
                fast_bindings.set(*symbol_id, tensor);
            }
        }

        for step in compiled {
            step.execute(foundry, &fast_bindings, bindings, &symbols)?;
        }

        Ok(())
    }

    fn compile(&self, bindings: &mut TensorBindings, symbols: &mut SymbolTable) -> Vec<Box<dyn CompiledStep>> {
        let input_idx = symbols.get_or_create(bindings.interpolate(self.input.0.clone()));
        let gamma_idx = symbols.get_or_create(bindings.interpolate(self.gamma.0.clone()));
        let wg_name = bindings.interpolate(self.wg.0.clone());
        let wu_name = bindings.interpolate(self.wu.0.clone());
        let wg_idx = symbols.get_or_create(wg_name.clone());
        let wg_scales_idx = symbols.get_or_create(format!("{wg_name}_scales"));
        let wu_idx = symbols.get_or_create(wu_name.clone());
        let wu_scales_idx = symbols.get_or_create(format!("{wu_name}_scales"));
        let bg_idx = self.bg.as_ref().map(|b| symbols.get_or_create(bindings.interpolate(b.0.clone())));
        let bu_idx = self.bu.as_ref().map(|b| symbols.get_or_create(bindings.interpolate(b.0.clone())));
        let out_idx = symbols.get_or_create(bindings.interpolate(self.out.0.clone()));

        vec![Box::new(CompiledFusedSwigluStep {
            step: self.clone(),
            input_idx,
            gamma_idx,
            wg_name: wg_name.clone(),
            wg_resolved: ResolvedSymbols {
                weights: wg_idx,
                scales: wg_scales_idx.into(),
                bias: None,
            },
            wu_name: wu_name.clone(),
            wu_resolved: ResolvedSymbols {
                weights: wu_idx,
                scales: wu_scales_idx.into(),
                bias: None,
            },
            bg_idx,
            bu_idx,
            out_idx,
        })]
    }
}

impl CompiledStep for CompiledFusedSwigluStep {
    fn execute(
        &self,
        foundry: &mut Foundry,
        fast_bindings: &FastBindings,
        bindings: &TensorBindings,
        _symbols: &SymbolTable,
    ) -> Result<(), MetalError> {
        let get = |idx| fast_bindings.get(idx).ok_or(MetalError::InputNotFound("".into()));

        let input = get(self.input_idx)?;
        let gamma = get(self.gamma_idx)?;
        let w_gate = get(self.wg_resolved.weights)?;
        let w_up = get(self.wu_resolved.weights)?;
        let output = get(self.out_idx)?;

        let (b_gate, has_b_gate) = if let Some(idx) = self.bg_idx {
            (TensorArg::from_tensor(get(idx)?), 1u32)
        } else {
            (TensorArg::from_tensor(output), 0u32)
        };

        let (b_up, has_b_up) = if let Some(idx) = self.bu_idx {
            (TensorArg::from_tensor(get(idx)?), 1u32)
        } else {
            (TensorArg::from_tensor(output), 0u32)
        };

        require_non_empty_io("FusedSwiglu", input, output)?;
        let k_dim = tail_dim(input)?;
        let n_dim = tail_dim(output)?;

        require_vector_len("gamma", &TensorArg::from_tensor(gamma), k_dim)?;
        if has_b_gate != 0 {
            require_vector_len("b_gate", &b_gate, n_dim)?;
        }
        if has_b_up != 0 {
            require_vector_len("b_up", &b_up, n_dim)?;
        }

        let policy_gate = crate::policy::resolve_policy(w_gate.dtype);
        let loader_gate = policy_gate.loader_stage();
        let args_gate = loader_gate.bind(fast_bindings, &self.wg_resolved);

        let policy_up = crate::policy::resolve_policy(w_up.dtype);
        let loader_up = policy_up.loader_stage();
        let args_up = loader_up.bind(fast_bindings, &self.wu_resolved);
        let batch = resolve_batch(bindings);

        // Safety path for mixed quantized Gate/Up policies.
        // A single fused kernel is specialized for one policy; running mixed tensors through it is incorrect.
        if w_gate.dtype != w_up.dtype {
            MIXED_SWIGLU_FALLBACK_WARN_ONCE.call_once(|| {
                tracing::warn!(
                    gate_dtype = ?w_gate.dtype,
                    up_dtype = ?w_up.dtype,
                    batch,
                    k_dim,
                    n_dim,
                    "FusedSwiglu mixed-policy fallback engaged; performance may regress until mixed-policy fused kernel support is added"
                );
            });
            let input_arg = TensorArg::from_tensor(input);
            let input_elems = input.dims.iter().product::<usize>();
            let normalized_buf = foundry
                .device
                .new_buffer(input_elems * std::mem::size_of::<f16>(), MetalResourceOptions::StorageModeShared)
                .ok_or_else(|| MetalError::OperationFailed("Failed to allocate mixed-swiglu normalized input".into()))?;
            let normalized = TensorArg::from_buffer(
                normalized_buf,
                crate::tensor::Dtype::F16,
                input.dims.to_vec(),
                input.strides.to_vec(),
            );
            let epsilon = resolve_rms_eps(bindings, Some(self.step.epsilon));
            crate::metals::rmsnorm::step::run_rmsnorm(
                foundry,
                &input_arg,
                None,
                &normalized,
                &TensorArg::from_tensor(gamma),
                crate::metals::rmsnorm::RmsNormParamsResolved {
                    feature_dim: k_dim,
                    total_elements: input_elems as u32,
                    epsilon,
                },
            )?;

            let output_elems = output.dims.iter().product::<usize>();
            let gate_out_buf = foundry
                .device
                .new_buffer(output_elems * std::mem::size_of::<f16>(), MetalResourceOptions::StorageModeShared)
                .ok_or_else(|| MetalError::OperationFailed("Failed to allocate mixed-swiglu gate output".into()))?;
            let gate_out = TensorArg::from_buffer(
                gate_out_buf,
                crate::tensor::Dtype::F16,
                output.dims.to_vec(),
                output.strides.to_vec(),
            );
            let up_out = TensorArg::from_tensor(output);

            run_canonical_projection(
                foundry,
                fast_bindings,
                policy_gate.clone(),
                &self.wg_resolved,
                &normalized,
                gate_out.clone(),
                k_dim,
                n_dim,
                batch,
                self.step.weights_per_block,
            )?;
            run_canonical_projection(
                foundry,
                fast_bindings,
                policy_up.clone(),
                &self.wu_resolved,
                &normalized,
                up_out.clone(),
                k_dim,
                n_dim,
                batch,
                self.step.weights_per_block,
            )?;

            let zero_bias_tensor = allocate_zero_bias(foundry, n_dim)?;
            let gate_bias_tensor = if has_b_gate != 0 {
                b_gate.clone()
            } else {
                TensorArg::from_tensor(&zero_bias_tensor)
            };
            let up_bias_tensor = if has_b_up != 0 {
                b_up.clone()
            } else {
                TensorArg::from_tensor(&zero_bias_tensor)
            };

            let swiglu = crate::metals::swiglu::Swiglu::new_auto_vectorized(
                &gate_out,
                &up_out,
                &gate_bias_tensor,
                &up_bias_tensor,
                output_elems as u32,
                n_dim,
                n_dim,
                n_dim,
            );
            foundry.run(&swiglu)?;
            return Ok(());
        }

        let w_gate_arg = args_gate[0].clone();
        let s_gate = if args_gate.len() > 1 { Some(args_gate[1].clone()) } else { None };
        let w_up_arg = args_up[0].clone();
        let s_up = if args_up.len() > 1 { Some(args_up[1].clone()) } else { None };
        let weights_per_block = effective_weights_per_block(policy_gate.as_ref(), self.step.weights_per_block);

        let args = super::ffn_step::FusedFfnArgs {
            w_gate: w_gate_arg,
            s_gate,
            w_up: w_up_arg,
            s_up,
            input: TensorArg::from_tensor(input),
            output: TensorArg::from_tensor(output),
            k_dim,
            n_dim,
            weights_per_block,
            gamma: TensorArg::from_tensor(gamma),
            b_gate,
            b_up,
            has_b_gate,
            has_b_up,
            epsilon: resolve_rms_eps(bindings, Some(self.step.epsilon)),
        };

        let kernel = get_fused_ffn_kernel(policy_gate);
        let dispatch = DispatchConfig::warp_per_row(n_dim, batch);

        foundry.run(&kernel.clone().bind_arc(args, dispatch))
    }

    fn name(&self) -> &'static str {
        "FusedSwiglu"
    }
}
