# 5.1 Quick Wins — Detailed Implementation Plan

Scope: Implement the items in 5.1 of GGML-METALLIC.md with a robust, idiomatic Rust design that enables zero-copy tensor use, strongly-typed dispatch, A/B testing, and clean developer experience for adding new kernels.

Primary goals
- Shape-aware matmul dispatcher with strongly-typed kernel variants and zero-copy handoffs.
- Consistent fused alpha/beta usage across matmul call sites.
- Prefer logical transpose for K in SDPA; avoid permutes.
- Parameterize softmax threadgroup sizing and introduce simdgroup reductions (when available).
- Extend pipeline cache keys to reflect specialization factors.
- Provide feature flags and test-only kernel registration for A/B experiments.
- Integrate instrumentation (with_gpu_scope) and memory/latency tracking.

Progress checklist (updated)
- [x] Define strongly typed enums/structs for dispatch (types.rs)
- [x] Implement dispatcher selection with exhaustive matches (dispatcher.rs) + unit tests
- [x] Env-based preferences loader and tests (prefs.rs)
- [x] Wired modules and passed fmt/clippy/build
- [ ] Implement execute_matmul handoff routing (preserve strides, fused alpha/beta, with_gpu_scope labels)
- [ ] Audit matmul_alpha_beta call sites for fused α/β usage and add parity tests
- [ ] Softmax selection scaffolding and parameterization
- [ ] Extend cache keys (transpose flags, beta!=0, small-N bucket, seq_k bucket, causal flag)
- [ ] SDPA logical transpose default + correctness tests
- [ ] Bench harness stubs and instrumentation labels
- [ ] Document env flags in README/docs

High-level design
- Strong types and enums for dispatch
  - Define `MatmulBackend` (MLX, MPS, Custom) and `MatmulVariant` (SmallN{n_bucket}, GemmSimdgroup{tile}, GemmGeneric).
  - Define `SoftmaxVariant` (Vec, Block) with policy traits to encapsulate TG sizing and reduction strategy.
  - Use exhaustive `match` for selection to avoid footguns; unit-test the selector.
- Zero-copy routing
  - All tensor views carry shape and stride metadata. Dispatch must honor non-compact strides and logical transposes.
  - Backends must accept strideful views or provide a contiguous fast-path if already contiguous. Avoid permutes/copies.
- Registry-based kernel manager (per docs/KERNELS.md)
  - Ensure `KernelManager` registers `matmul_dispatch` entry; dispatcher does not itself implement math, only selection and handoff to `matmul_*` kernels.
  - A `KernelRegistry` mapping `(DType, VariantKey)` -> `KernelHandle` loaded via `KernelManager`.
  - Test-only kernels may be registered behind a `#[cfg(any(test, feature = "exp_kernels"))]` gate.
- Feature flags / Env vars
  - `METALLIC_MATMUL_BACKEND={mlx|mps|custom|auto}`
  - `METALLIC_MATMUL_VARIANT_FORCE={smalln|gemm|auto}` and `METALLIC_SMALLN_MAX_N=8` (tune via benches)
  - `METALLIC_SOFTMAX_VARIANT={vec|block|auto}`
  - These allow A/B testing and quick rollback.

Public API surfaces (unchanged semantics)
- Keep existing `matmul_alpha_beta(...)` signature and SDPA APIs stable.
- Introduce a kernel entrypoint `matmul_dispatch` so callers can `ctx.call<matmul_dispatch>(...)` (per docs/KERNELS.md pattern). Internally it selects and invokes one of `matmul_mlx`, `matmul_mps`, or `matmul_gemv`.
- Allow callers to pass transpose flags; prefer logical transpose handling.

Detailed steps

1) Types and enums
- Add in `crates/metallic/src/kernels/matmul_dispatcher/`: 
  - `pub enum MatmulBackend { Auto, MLX, MPS, Custom }`
  - `pub enum SmallNBucket { N1, N2, N4, N8, N16, Other }`
  - `pub enum GemmTile { T64x32xK, T64x64xK, Generic }`
  - `pub enum MatmulVariant { SmallN(SmallNBucket), GemmSimd(GemmTile), GemmGeneric }`
  - `pub struct MatmulCaps { has_simdgroup_mm: bool, device_family: DeviceFamily }`
  - `pub struct MatShape { m: usize, k: usize, n: usize }`
  - `pub struct MatmulPolicy { backend: MatmulBackend, variant: MatmulVariant }`
- Implement `From<usize> for SmallNBucket` to bucketize N.
- Implement `Display` / `Debug` where helpful.

2) Dispatcher
- New module: `matmul_dispatcher/dispatcher.rs` with:
  - `pub fn select_policy(shape: MatShape, dtype: DType, caps: &MatmulCaps, prefs: &Prefs) -> MatmulPolicy`
  - Selection order:
    - If prefs.force_backend != Auto -> backend = forced
    - If prefs.force_variant == SmallN and N<=16 -> SmallN bucket
    - Else if N<=8 -> SmallN bucket
    - Else if caps.has_simdgroup_mm && shape.m >= 64 && shape.n >= 16 -> GemmSimd(default tile)
    - Else -> GemmGeneric
  - Resolve backend:
    - Prefer MLX for GemmGeneric if available; else MPS; allow Custom when variant requires it.
  - Use exhaustive match to construct a `DispatchPlan`:
    - `enum DispatchPlan { UseMLX(MatmulVariant), UseMPS(MatmulVariant), UseCustom(MatmulVariant) }`
- Add unit tests for key shapes and env-forced cases.

3) Handoff layer
- Implement `execute_matmul(plan: DispatchPlan, args: MatmulArgs) -> Result<()>` in `matmul_dispatcher/execute.rs` and route to `matmul_mlx`, `matmul_mps`, or `matmul_gemv` accordingly that:
  - Preserves strides and transpose flags, preferring logical transpose.
  - Calls MLX/MPS/Custom entry points.
  - Ensures fused alpha/beta is honored.
  - Uses `with_gpu_scope!("matmul", labels... )` to label variant and shape.
- Validate zero-copy by asserting no intermediate allocations unless strictly needed.

4) MLX/MPS path audit for alpha/beta fusion
- Review callers of `matmul_alpha_beta` and ensure the fused path is used across call sites (QK, AV, feed-forward).
- Add tests that compare result of fused call vs unfused composition for numerical parity.

5) Custom Small-N kernel stub integration
- Define trait:
  - `pub trait CustomMatmulKernel { fn launch(&self, args: &MatmulArgs) -> Result<()>; fn key(&self) -> VariantKey; }`
- Provide placeholder registration (no-op kernel that calls MLX temporarily) under `#[cfg(any(test, feature = "exp_kernels"))]`:
  - This enables wiring of selection and benchmarking before real kernel exists.
- Bench-only kernels can be registered via a registry macro in tests.

6) Cache key extensions
- Extend `ResourceCache`/`KernelManager` keys to include:
  - Transpose flags for A/B, β!=0 marker, SmallN bucket, Gemm tile, dtype.
  - For SDPA softmax: seq_k bucket, causal flag, reduction impl (simdgroup/shared).
- Add tests to ensure different keys don’t collide.

7) Softmax parameterization (foundation only in 5.1)
- Introduce `SoftmaxVariant` enum { Vec, Block, Auto } and `SoftmaxPrefs` from env.
- Parameterize threadgroup size by nearest pow2 <= seq_k, min(device_limit).
- Optional simdgroup reduction path behind capability flag (implementation can land in 5.2; 5.1 wires selection and params).
- Ensure `with_gpu_scope!("softmax", ...)` records variant and tg size.

8) SDPA logical transpose preference
- Modify SDPA path to prefer transpose_k=true and pass K with transposed strides when backend supports it (MLX path).
- Add guard to avoid materialized permutes; add correctness tests for strided/transposed reads.

9) Instrumentation & metrics
- Ensure each dispatch path emits:
  - backend, variant, dtype, shape, strides, alpha/beta flags.
- Integrate memory accounting: bytes read/written per kernel (approx) recorded for profiling builds.
- Add timing scopes around each kernel call.

10) A/B testing protocol
- Env flag to force variant/backend.
- Hidden test-only kernels under `exp_kernels` feature.
- Bench harness reads env and runs shape sweeps, outputs CSV.
- CI job (optional) to compare against baseline thresholds.

11) Tests
- Unit tests:
  - Dispatcher shape cases and env overrides (exhaustive for small matrix shapes).
  - Fused alpha/beta parity tests.
  - SDPA K-transpose logical-view tests.
  - Cache key uniqueness tests.
- Integration tests:
  - End-to-end matmul for representative shapes (small-N, medium, large) with synchronization.
- Property tests (optional):
  - Random shapes within constraints, verify numeric bounds.

12) Benchmarks
- Add/extend benches:
  - `benches/matmul_smalln_bench.rs`: M∈{128,512,2048}, N∈{1,2,4,8,16}, K∈{1024,2048,4096}, batched strides.
  - `benches/sdpa_smoke_bench.rs`: focus on QK/AV matmul legs for now, record softmax params.
- Emit per-variant metrics and write CSV; include device info.

13) Rollout & flags
- Default: Auto dispatcher with MLX for GemmGeneric; Custom SmallN path disabled until kernel lands.
- Feature flags allow canary enabling of SmallN once implemented.
- Document env flags in README/docs.

14) Skeleton code snippets

Dispatcher selection skeleton
```rust
pub fn select_policy(shape: MatShape, caps: &MatmulCaps, prefs: &Prefs) -> DispatchPlan {
    let n_bucket = SmallNBucket::from(shape.n);
    match (prefs.backend, prefs.variant) {
        (MatmulBackend::MLX, _) => DispatchPlan::UseMLX(match n_bucket {
            SmallNBucket::N1|SmallNBucket::N2|SmallNBucket::N4|SmallNBucket::N8 if prefs.variant_allows_smalln() => MatmulVariant::SmallN(n_bucket),
            _ if caps.has_simdgroup_mm && shape.m >= 64 && shape.n >= 16 => MatmulVariant::GemmSimd(GemmTile::T64x32xK),
            _ => MatmulVariant::GemmGeneric,
        }),
        (MatmulBackend::MPS, _) => DispatchPlan::UseMPS(MatmulVariant::GemmGeneric),
        (MatmulBackend::Custom, _) => DispatchPlan::UseCustom(match n_bucket {
            SmallNBucket::N1|SmallNBucket::N2|SmallNBucket::N4|SmallNBucket::N8 => MatmulVariant::SmallN(n_bucket),
            _ if caps.has_simdgroup_mm => MatmulVariant::GemmSimd(GemmTile::T64x32xK),
            _ => MatmulVariant::GemmGeneric,
        }),
        (MatmulBackend::Auto, _) => {
            if prefs.variant_allows_smalln() && matches!(n_bucket, SmallNBucket::N1|SmallNBucket::N2|SmallNBucket::N4|SmallNBucket::N8) {
                DispatchPlan::UseCustom(MatmulVariant::SmallN(n_bucket))
            } else if caps.has_simdgroup_mm && shape.m >= 64 && shape.n >= 16 {
                DispatchPlan::UseCustom(MatmulVariant::GemmSimd(GemmTile::T64x32xK))
            } else {
                DispatchPlan::UseMLX(MatmulVariant::GemmGeneric)
            }
        }
    }
}
```

Matmul execution skeleton
```rust
pub fn execute_matmul(plan: DispatchPlan, args: &MatmulArgs) -> Result<()> {
    with_gpu_scope!("matmul", || {
        match plan {
            DispatchPlan::UseMLX(var) => mlx::execute(var, args),
            DispatchPlan::UseMPS(var) => mps::execute(var, args),
            DispatchPlan::UseCustom(var) => custom::execute(var, args),
        }
    })
}
```

Softmax parameterization skeleton
```rust
pub enum SoftmaxVariant { Auto, Vec, Block }

pub fn select_softmax_variant(seq_k: usize, caps: &Caps, prefs: &SoftmaxPrefs) -> (SoftmaxVariant, usize) {
    if let Some(forced) = prefs.forced_variant { return (forced, prefs.forced_tg_size.unwrap_or(default_tg())); }
    let tg = nearest_pow2_bounded(seq_k, caps.max_tg_size);
    if seq_k <= 1024 { (SoftmaxVariant::Vec, tg) } else { (SoftmaxVariant::Block, tg) }
}
```

15) Risks and mitigations
- Risk: dispatcher complexity -> Mitigate by exhaustive enums, unit tests, and clear logging.
- Risk: zero-copy assumptions broken -> Validate strides and assert on mismatches; fall back with metrics when needed.
- Risk: cache key explosion -> Bucketize sizes (N, seq_k) and keep dtype/flags; test collisions.
- Risk: performance regressions -> A/B gates and benchmark thresholds; default to known-fast MLX.

16) Definition of Done (DoD)
- Dispatcher in place with unit tests and env flags.
- Fused alpha/beta audit complete and parity tests added.
- Softmax selection wired with parameterization (implementation may be naive, variant infra present).
- SDPA logical transpose defaulted and correctness tested.
- Cache keys extended and tested.
- Benches compile and run locally; instrumentation emits variant labels.
- cargo fmt, clippy, and build pass.

17) Task breakdown (PR-friendly)
- PR1: Types/enums, dispatcher selection, env flags, tests.
- PR2: Handoff layer, MLX/MPS audit for fused alpha/beta, tests.
- PR3: Softmax variant selection + param wiring, tests.
- PR4: SDPA logical transpose preference + tests.
- PR5: Cache key extensions + registry wiring + tests.
- PR6: Bench harnesses and instrumentation labels.
- PR7+: Introduce first experimental SmallN kernel behind `exp_kernels` (to be done under 5.2 work but can be scaffolded here).
