# 5.3 Implementation Plan: Performance Tuning & Next-Gen Kernels

Scope: This document outlines the plan for the 5.3 development cycle. It includes large strategic goals outlined in `GGML-METALLIC.md`. The primary focus is on empirical tuning, new kernel development, and architectural enhancements for future capabilities.

We plan to return to 5.1 and 5.2 items once we're done with 5.3 as we feel further tuning is only worthwhile once we've solidified our available kernels.

## New Kernel Development

This phase focuses on implementing the "Larger Tasks" from `GGML-METALLIC.md` to close the performance gap with GGML on larger matrix operations.

### 2.1. Tiled GEMM Kernel for Large M, N
- [x] **Implement `matmul_gemm_tiled` Kernel**:
  - **Task**: Create a new Metal kernel file for a generic, tiled GEMM implementation.
  - **Task**: Implement a `simdgroup_matrix_multiply` variant within the kernel, gated by `MTLDevice` capability checks at runtime.
  - **Task**: Provide a fallback path using threadgroup memory and SIMD-group reductions for devices without hardware matrix multiplication support.
- [x] **Benchmark and Tune Tile Sizes**:
  - **Task**: Benchmark various tile configurations (e.g., `tile_m`, `tile_n`, `tile_k`) to find the optimal balance of threadgroup memory usage, register pressure, and occupancy for each device family (M1/M2/M3).
- [x] **Integrate into MatmulDispatcher**:
  - **Task**: Add `MatmulVariant::GemmTiled` and integrate the new kernel into the `MatmulDispatcher`. It should be selected for large `M` and `N` shapes where it outperforms the `MLX` fallback.

### 2.2. Cross-Cutting Architectural Improvements
- [ ] **Vectorized Memory Access Audit**:
  - **Task**: Systematically review all major kernels (`gemv`, `softmax`, and the new `gemm_tiled`) to use vectorized loads/stores (`half2`/`half4`) wherever possible.
  - **Task**: Ensure all memory access includes alignment checks with a scalar fallback path to maintain correctness.
- [ ] **Embrace Strided Views**:
  - **Task**: Continue to refactor kernels to efficiently handle non-contiguous tensor views via strides, further reducing the need for explicit `permute` or `contiguous` calls.
  - **Task**: The goal is to make zero-copy, stride-based operations the default, not the exception.

## Phase 3: 5.4 and Beyond â€” Future-Proofing

These tasks are strategic and lay the groundwork for future capabilities like quantization and more advanced attention mechanisms.

### 3.1. Quantization-Aware Matmul Pipelines
- [ ] **Design Quantized Kernel Interfaces**:
  - **Task**: Refactor the `CustomMatmulKernel` trait and `MatmulArgs` to gracefully handle quantized tensors (e.g., Q4_0, Q8_0). This includes passing scale, offset, and zero-point parameters.
  - **Task**: This is a design-focused task. The goal is to create an abstraction that allows plugging in new quantization kernels without requiring changes to high-level model code.
- [ ] **Implement a Placeholder Quantized Kernel**:
  - **Task**: Implement a simple de-quantizing GEMV kernel (e.g., Q8_0 -> F16) as a proof-of-concept for the new interface.

### 3.2. Advanced Attention Pipeline Research
- [ ] **Investigate Fused QK-Softmax**:
  - **Task**: Research and (optionally) prototype a "lightly fused" kernel that combines the QK^T matmul and the softmax operation for small sequences.
  - **Task**: Create a benchmark to compare this fused approach against the current discrete three-stage SDPA pipeline to evaluate the trade-off between reduced memory traffic and increased kernel complexity.
