{
    "name": "qwen2.5-0.5b",
    "architecture": {
        "d_model": 896,
        "n_heads": 14,
        "n_kv_heads": 2,
        "n_layers": 24,
        "ff_dim": 4864,
        "vocab_size": 151936,
        "max_seq_len": 32768,
        "rope_base": 1000000.0,
        "rms_eps": 1e-06,
        "tensor_names": {
            "embedding": [
                "token_embd.weight",
                "model.embed_tokens.weight"
            ],
            "output_weight": [
                "output.weight",
                "lm_head.weight",
                "token_embd.weight"
            ],
            "final_norm": [
                "output_norm.weight",
                "model.norm.weight"
            ],
            "rope_cos": [
                "rope_cos",
                "model.rotary.cos"
            ],
            "rope_sin": [
                "rope_sin",
                "model.rotary.sin"
            ],
            "layer": {
                "attn_norm": [
                    "blk.{i}.attn_norm.weight",
                    "model.layers.{i}.input_layernorm.weight"
                ],
                "ffn_norm": [
                    "blk.{i}.ffn_norm.weight",
                    "model.layers.{i}.post_attention_layernorm.weight"
                ],
                "attn_q": [
                    "blk.{i}.attn_q.weight",
                    "model.layers.{i}.self_attn.q_proj.weight"
                ],
                "attn_k": [
                    "blk.{i}.attn_k.weight",
                    "model.layers.{i}.self_attn.k_proj.weight"
                ],
                "attn_v": [
                    "blk.{i}.attn_v.weight",
                    "model.layers.{i}.self_attn.v_proj.weight"
                ],
                "attn_q_bias": [
                    "blk.{i}.attn_q.bias",
                    "model.layers.{i}.self_attn.q_proj.bias"
                ],
                "attn_k_bias": [
                    "blk.{i}.attn_k.bias",
                    "model.layers.{i}.self_attn.k_proj.bias"
                ],
                "attn_v_bias": [
                    "blk.{i}.attn_v.bias",
                    "model.layers.{i}.self_attn.v_proj.bias"
                ],
                "attn_output": [
                    "blk.{i}.attn_output.weight",
                    "model.layers.{i}.self_attn.o_proj.weight"
                ],
                "ffn_gate": [
                    "blk.{i}.ffn_gate.weight",
                    "model.layers.{i}.mlp.gate_proj.weight"
                ],
                "ffn_up": [
                    "blk.{i}.ffn_up.weight",
                    "model.layers.{i}.mlp.up_proj.weight"
                ],
                "ffn_down": [
                    "blk.{i}.ffn_down.weight",
                    "model.layers.{i}.mlp.down_proj.weight"
                ],
                "ffn_gate_bias": [
                    "blk.{i}.ffn_gate.bias",
                    "model.layers.{i}.mlp.gate_proj.bias"
                ],
                "ffn_up_bias": [
                    "blk.{i}.ffn_up.bias",
                    "model.layers.{i}.mlp.up_proj.bias"
                ],
                "ffn_down_bias": [
                    "blk.{i}.ffn_down.bias",
                    "model.layers.{i}.mlp.down_proj.bias"
                ]
            }
        },
        "forward": [
            {
                "op": "Embedding",
                "table": "embedding",
                "indices": "input_ids",
                "output": "hidden",
                "params": {
                    "d_model": 896,
                    "total_elements": "{total_elements_hidden}",
                    "vocab_size": 151936
                }
            },
            {
                "op": "Repeat",
                "count": "n_layers",
                "var": "i",
                "steps": [
                    {
                        "op": "FusedQkv",
                        "input": "hidden",
                        "gamma": "layer.attn_norm_{i}",
                        "w_q": "layer.attn_q_{i}",
                        "w_k": "layer.attn_k_{i}",
                        "w_v": "layer.attn_v_{i}",
                        "bias_q": "layer.attn_q_bias_{i}",
                        "bias_k": "layer.attn_k_bias_{i}",
                        "bias_v": "layer.attn_v_bias_{i}",
                        "out_q": "q",
                        "out_k": "k",
                        "out_v": "v",
                        "k_dim": "{d_model}",
                        "n_dim": "{d_model}",
                        "n_kv": "{kv_dim}",
                        "weights_per_block": 32,
                        "m": "{m}",
                        "strategy": "Vectorized"
                    },
                    {
                        "op": "KvRearrange",
                        "input": "q",
                        "output": "q_heads",
                        "params": {
                            "kv_dim": "{d_model}",
                            "row_stride": "{d_model}",
                            "kv_head_dim": "{head_dim}",
                            "n_heads": "{n_heads}",
                            "n_kv_heads": "{n_heads}",
                            "head_dim": "{head_dim}",
                            "seq": "{seq_len}",
                            "total_elements": "{total_elements_q}"
                        }
                    },
                    {
                        "op": "KvRearrange",
                        "input": "k",
                        "output": "k_heads",
                        "params": {
                            "kv_dim": "{kv_dim}",
                            "row_stride": "{kv_dim}",
                            "kv_head_dim": "{head_dim}",
                            "n_heads": "{n_kv_heads}",
                            "n_kv_heads": "{n_kv_heads}",
                            "head_dim": "{head_dim}",
                            "seq": "{seq_len}",
                            "total_elements": "{total_elements_k}"
                        }
                    },
                    {
                        "op": "KvRearrange",
                        "input": "v",
                        "output": "v_heads",
                        "params": {
                            "kv_dim": "{kv_dim}",
                            "row_stride": "{kv_dim}",
                            "kv_head_dim": "{head_dim}",
                            "n_heads": "{n_kv_heads}",
                            "n_kv_heads": "{n_kv_heads}",
                            "head_dim": "{head_dim}",
                            "seq": "{seq_len}",
                            "total_elements": "{total_elements_k}"
                        }
                    },
                    {
                        "op": "Rope",
                        "input": "q_heads",
                        "output": "q_rot",
                        "cos": "rope_cos",
                        "sin": "rope_sin",
                        "params": {
                            "dim": 64,
                            "seq_len": "{seq_len}",
                            "position_offset": "{position_offset}",
                            "total_elements": "{total_elements_q}"
                        }
                    },
                    {
                        "op": "Rope",
                        "input": "k_heads",
                        "output": "k_rot",
                        "cos": "rope_cos",
                        "sin": "rope_sin",
                        "params": {
                            "dim": 64,
                            "seq_len": "{seq_len}",
                            "position_offset": "{position_offset}",
                            "total_elements": "{total_elements_k}"
                        }
                    },
                    {
                        "op": "KvCacheWriteRepeatKvHeads",
                        "input": "k_rot",
                        "cache": "k_cache_{i}",
                        "params": {
                            "n_kv_heads": 2,
                            "n_heads": 14,
                            "group_size": 7,
                            "head_dim": 64,
                            "input_seq_len": "{seq_len}",
                            "max_seq_len": "{max_seq_len}",
                            "total_elements": "{total_elements_write}",
                            "position_offset": "{position_offset}",
                            "layer_idx": "{i}"
                        }
                    },
                    {
                        "op": "KvCacheWriteRepeatKvHeads",
                        "input": "v_heads",
                        "cache": "v_cache_{i}",
                        "params": {
                            "n_kv_heads": 2,
                            "n_heads": 14,
                            "group_size": 7,
                            "head_dim": 64,
                            "input_seq_len": "{seq_len}",
                            "max_seq_len": "{max_seq_len}",
                            "total_elements": "{total_elements_write}",
                            "position_offset": "{position_offset}",
                            "layer_idx": "{i}"
                        }
                    },
                    {
                        "op": "SdpaMaterialized",
                        "q": "q_rot",
                        "k": "k_cache_{i}",
                        "v": "v_cache_{i}",
                        "output": "attn_out",
                        "causal": true,
                        "query_offset": "{position_offset}",
                        "n_heads": 14,
                        "head_dim": 64,
                        "kv_seq_len": "{kv_seq_len}",
                        "m": "{m}",
                        "kv_head_major": true
                    },
                    {
                        "op": "MatMul",
                        "m": "{m}",
                        "n": "{d_model}",
                        "k": "{d_model}",
                        "transpose_b": true,
                        "b": "layer.attn_output_{i}",
                        "b_scales": "layer.attn_output_{i}",
                        "a": "attn_out",
                        "output": "residual_1",
                        "params": {
                            "batch": 1,
                            "weights_per_block": 32
                        },
                        "bias": "zero",
                        "c": "hidden",
                        "alpha": 1.0,
                        "beta": 1.0,
                        "has_bias": 0
                    },
                    {
                        "op": "FusedFfnSwiGluRmsNorm",
                        "input": "residual_1",
                        "gamma": "layer.ffn_norm_{i}",
                        "w_gate": "layer.ffn_gate_{i}",
                        "w_up": "layer.ffn_up_{i}",
                        "b_gate": "layer.ffn_gate_bias_{i}",
                        "b_up": "layer.ffn_up_bias_{i}",
                        "output": "up",
                        "weights_per_block": 32
                    },
                    {
                        "op": "MatMul",
                        "m": "{m}",
                        "n": "{d_model}",
                        "k": "{ff_dim}",
                        "transpose_b": true,
                        "b": "layer.ffn_down_{i}",
                        "b_scales": "layer.ffn_down_{i}",
                        "a": "up",
                        "output": "hidden",
                        "params": {
                            "batch": 1,
                            "weights_per_block": 32
                        },
                        "bias": "layer.ffn_down_bias_{i}",
                        "c": "residual_1",
                        "alpha": 1.0,
                        "beta": 1.0,
                        "has_bias": 1
                    }
                ]
            },
            {
                "op": "RmsNormV2",
                "input": "hidden",
                "output": "final_norm_out",
                "gamma": "final_norm",
                "feature_dim": 896,
                "total_elements": "{total_elements_hidden}"
            },
            {
                "op": "MatMul",
                "m": "{m}",
                "n": "{vocab_size}",
                "k": "{d_model}",
                "transpose_b": true,
                "b": "output_weight",
                "b_scales": "output_weight",
                "a": "final_norm_out",
                "output": "logits",
                "params": {
                    "batch": 1,
                    "weights_per_block": 32
                },
                "bias": "zero",
                "c": "zero",
                "alpha": 1.0,
                "beta": 0.0,
                "has_bias": 0
            }
        ]
    }
}
