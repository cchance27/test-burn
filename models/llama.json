{
    "name": "llama3",
    "architecture": {
        "metadata_keys": {
            "keys": {
                "d_model": [
                    "llama.embedding_length",
                    "llama.d_model",
                    "model.d_model"
                ],
                "n_heads": [
                    "llama.attention.head_count"
                ],
                "n_kv_heads": [
                    "llama.attention.head_count_kv"
                ],
                "n_layers": [
                    "llama.block_count"
                ],
                "ff_dim": [
                    "llama.feed_forward_length"
                ],
                "max_seq_len": [
                    "llama.context_length"
                ],
                "vocab_size": [
                    "llama.vocab_size",
                    "model.vocab_size",
                    "@len:tokenizer.ggml.tokens"
                ],
                "rope_base": [
                    "llama.rope.freq_base"
                ],
                "rms_eps": [
                    "llama.attention.layer_norm_rms_epsilon"
                ]
            }
        },
        "tensor_names": {
            "embedding": [
                "token_embd.weight",
                "model.embed_tokens.weight"
            ],
            "output_weight": [
                "output.weight",
                "lm_head.weight",
                "token_embd.weight"
            ],
            "final_norm": [
                "output_norm.weight",
                "model.norm.weight"
            ],
            "rope_cos": [
                "rope_cos",
                "model.rotary.cos"
            ],
            "rope_sin": [
                "rope_sin",
                "model.rotary.sin"
            ],
            "layer": {
                "attn_norm": [
                    "blk.{i}.attn_norm.weight",
                    "model.layers.{i}.input_layernorm.weight"
                ],
                "ffn_norm": [
                    "blk.{i}.ffn_norm.weight",
                    "model.layers.{i}.post_attention_layernorm.weight"
                ],
                "attn_q": [
                    "blk.{i}.attn_q.weight",
                    "model.layers.{i}.self_attn.q_proj.weight"
                ],
                "attn_k": [
                    "blk.{i}.attn_k.weight",
                    "model.layers.{i}.self_attn.k_proj.weight"
                ],
                "attn_v": [
                    "blk.{i}.attn_v.weight",
                    "model.layers.{i}.self_attn.v_proj.weight"
                ],
                "attn_q_bias": [
                    "blk.{i}.attn_q.bias",
                    "model.layers.{i}.self_attn.q_proj.bias"
                ],
                "attn_k_bias": [
                    "blk.{i}.attn_k.bias",
                    "model.layers.{i}.self_attn.k_proj.bias"
                ],
                "attn_v_bias": [
                    "blk.{i}.attn_v.bias",
                    "model.layers.{i}.self_attn.v_proj.bias"
                ],
                "attn_output": [
                    "blk.{i}.attn_output.weight",
                    "model.layers.{i}.self_attn.o_proj.weight"
                ],
                "ffn_gate": [
                    "blk.{i}.ffn_gate.weight",
                    "model.layers.{i}.mlp.gate_proj.weight"
                ],
                "ffn_up": [
                    "blk.{i}.ffn_up.weight",
                    "model.layers.{i}.mlp.up_proj.weight"
                ],
                "ffn_down": [
                    "blk.{i}.ffn_down.weight",
                    "model.layers.{i}.mlp.down_proj.weight"
                ],
                "ffn_gate_bias": [
                    "blk.{i}.ffn_gate.bias",
                    "model.layers.{i}.mlp.gate_proj.bias"
                ],
                "ffn_up_bias": [
                    "blk.{i}.ffn_up.bias",
                    "model.layers.{i}.mlp.up_proj.bias"
                ],
                "ffn_down_bias": [
                    "blk.{i}.ffn_down.bias",
                    "model.layers.{i}.mlp.down_proj.bias"
                ]
            }
        },
        "prepare": {
            "globals": {},
            "dynamics": {
                "m": 1,
                "seq_len": 1,
                "position_offset": 0
            },
            "derived_globals": [
                {
                    "name": "head_dim",
                    "expr": "d_model / n_heads"
                },
                {
                    "name": "kv_dim",
                    "expr": "head_dim * n_kv_heads"
                },
                {
                    "name": "group_size",
                    "expr": "n_heads / n_kv_heads"
                },
                {
                    "name": "kv_seq_len",
                    "expr": "position_offset + seq_len"
                },
                {
                    "name": "total_elements_hidden",
                    "expr": "m * d_model"
                },
                {
                    "name": "total_elements_q",
                    "expr": "m * n_heads * head_dim"
                },
                {
                    "name": "total_elements_k",
                    "expr": "m * n_kv_heads * head_dim"
                },
                {
                    "name": "total_elements_write",
                    "expr": "m * n_kv_heads * head_dim"
                },
                {
                    "name": "total_elements_ffn",
                    "expr": "m * ff_dim"
                },
                {
                    "name": "total_elements_slice",
                    "expr": "n_kv_heads * kv_seq_len * head_dim"
                },
                {
                    "name": "total_elements_repeat",
                    "expr": "n_heads * kv_seq_len * head_dim"
                }
            ],
            "tensors": [
                {
                    "name": "hidden",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        "max_prefill_chunk",
                        "d_model"
                    ]
                },
                {
                    "name": "q",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        1,
                        "max_prefill_chunk",
                        "d_model"
                    ]
                },
                {
                    "name": "k",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        1,
                        "max_prefill_chunk",
                        "kv_dim"
                    ]
                },
                {
                    "name": "v",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        1,
                        "max_prefill_chunk",
                        "kv_dim"
                    ]
                },
                {
                    "name": "q_rot",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        1,
                        "max_prefill_chunk",
                        "d_model"
                    ]
                },
                {
                    "name": "attn_out",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        "max_prefill_chunk",
                        "d_model"
                    ]
                },
                {
                    "name": "residual_1",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        "max_prefill_chunk",
                        "d_model"
                    ]
                },
                {
                    "name": "up",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        "max_prefill_chunk",
                        "ff_dim"
                    ]
                },
                {
                    "name": "final_norm_out",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        "max_prefill_chunk",
                        "d_model"
                    ]
                },
                {
                    "name": "logits",
                    "dtype": "F16",
                    "storage": "intermediate",
                    "dims": [
                        "max_prefill_chunk",
                        "vocab_size"
                    ]
                },
                {
                    "name": "zero",
                    "dtype": "F16",
                    "storage": "shared",
                    "dims": [
                        1
                    ],
                    "zero_fill": true
                },
                {
                    "name": "rope_cos",
                    "dtype": "F16",
                    "storage": "rope_cache",
                    "dims": [
                        "max_seq_len",
                        "head_dim / 2"
                    ],
                    "grow_with_kv": true
                },
                {
                    "name": "rope_sin",
                    "dtype": "F16",
                    "storage": "rope_cache",
                    "dims": [
                        "max_seq_len",
                        "head_dim / 2"
                    ],
                    "grow_with_kv": true
                },
                {
                    "name": "k_cache_{i}",
                    "repeat": {
                        "count": "n_layers",
                        "var": "i"
                    },
                    "dtype": "F16",
                    "storage": "kv_cache",
                    "dims": [
                        "n_heads",
                        "max_seq_len",
                        "head_dim"
                    ],
                    "grow_with_kv": true
                },
                {
                    "name": "v_cache_{i}",
                    "repeat": {
                        "count": "n_layers",
                        "var": "i"
                    },
                    "dtype": "F16",
                    "storage": "kv_cache",
                    "dims": [
                        "n_heads",
                        "max_seq_len",
                        "head_dim"
                    ],
                    "grow_with_kv": true
                }
            ],
            "rope": {
                "cos": "rope_cos",
                "sin": "rope_sin"
            }
        },
        "weight_bindings": [
            {
                "key": "embedding",
                "logical_name": "embedding"
            },
            {
                "key": "final_norm",
                "logical_name": "final_norm"
            },
            {
                "key": "output_weight",
                "logical_name": "output_weight"
            },
            {
                "key": "layer.attn_norm",
                "logical_name": "layer.attn_norm_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                }
            },
            {
                "key": "layer.ffn_norm",
                "logical_name": "layer.ffn_norm_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                }
            },
            {
                "key": "layer.attn_q",
                "logical_name": "layer.attn_q_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                },
                "layout": {
                    "kind": "canonical",
                    "expected_k": "d_model",
                    "expected_n": "d_model"
                }
            },
            {
                "key": "layer.attn_k",
                "logical_name": "layer.attn_k_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                },
                "layout": {
                    "kind": "canonical",
                    "expected_k": "d_model",
                    "expected_n": "kv_dim"
                }
            },
            {
                "key": "layer.attn_v",
                "logical_name": "layer.attn_v_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                },
                "layout": {
                    "kind": "canonical",
                    "expected_k": "d_model",
                    "expected_n": "kv_dim"
                }
            },
            {
                "key": "layer.attn_output",
                "logical_name": "layer.attn_output_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                }
            },
            {
                "key": "layer.ffn_gate",
                "logical_name": "layer.ffn_gate_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                }
            },
            {
                "key": "layer.ffn_up",
                "logical_name": "layer.ffn_up_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                }
            },
            {
                "key": "layer.ffn_down",
                "logical_name": "layer.ffn_down_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                }
            },
            {
                "key": "layer.attn_q_bias",
                "logical_name": "layer.attn_q_bias_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                },
                "fallback_zero_len": "d_model"
            },
            {
                "key": "layer.attn_k_bias",
                "logical_name": "layer.attn_k_bias_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                },
                "fallback_zero_len": "kv_dim"
            },
            {
                "key": "layer.attn_v_bias",
                "logical_name": "layer.attn_v_bias_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                },
                "fallback_zero_len": "kv_dim"
            },
            {
                "key": "layer.ffn_gate_bias",
                "logical_name": "layer.ffn_gate_bias_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                },
                "fallback_zero_len": "ff_dim"
            },
            {
                "key": "layer.ffn_up_bias",
                "logical_name": "layer.ffn_up_bias_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                },
                "fallback_zero_len": "ff_dim"
            },
            {
                "key": "layer.ffn_down_bias",
                "logical_name": "layer.ffn_down_bias_{i}",
                "repeat": {
                    "count": "n_layers",
                    "var": "i"
                },
                "fallback_zero_len": "d_model"
            }
        ],
        "forward": [
            {
                "op": "Embedding",
                "table": "embedding",
                "indices": "input_ids",
                "output": "hidden",
                "params": {
                    "d_model": "{d_model}",
                    "total_elements": "{total_elements_hidden}",
                    "vocab_size": "{vocab_size}"
                }
            },
            {
                "op": "Repeat",
                "count": "n_layers",
                "var": "i",
                "steps": [
                    {
                        "op": "FusedQkv",
                        "input": "hidden",
                        "gamma": "layer.attn_norm_{i}",
                        "w_q": "layer.attn_q_{i}",
                        "w_k": "layer.attn_k_{i}",
                        "w_v": "layer.attn_v_{i}",
                        "bias_q": "layer.attn_q_bias_{i}",
                        "bias_k": "layer.attn_k_bias_{i}",
                        "bias_v": "layer.attn_v_bias_{i}",
                        "out_q": "q",
                        "out_k": "k",
                        "out_v": "v",
                        "k_dim": "{d_model}",
                        "n_dim": "{d_model}",
                        "n_kv": "{kv_dim}",
                        "weights_per_block": 32,
                        "m": "{m}",
                        "strategy": "Vectorized"
                    },
                    {
        "op": "KvPrepFused",
        "q": "q",
        "k": "k",
        "v": "v",
                        "q_rot": "q_rot",
                        "k_cache": "k_cache_{i}",
                        "v_cache": "v_cache_{i}",
                        "cos": "rope_cos",
                        "sin": "rope_sin",
                        "params": {
                            "d_model": "{d_model}",
                            "kv_dim": "{kv_dim}",
                            "head_dim": "{head_dim}",
                            "n_heads": "{n_heads}",
                            "n_kv_heads": "{n_kv_heads}",
                            "group_size": "{group_size}",
                            "seq_len": "{seq_len}",
                            "position_offset": "{position_offset}",
                            "max_seq_len": "{max_seq_len}",
                            "total_elements": "{total_elements_q}"
                        }
                    },
                    {
                        "op": "FlashAttention",
                        "q": "q_rot",
                        "k": "k_cache_{i}",
                        "v": "v_cache_{i}",
                        "output": "attn_out",
                        "causal": true,
                        "query_offset": "{position_offset}",
                        "n_heads": "{n_heads}",
                        "head_dim": "{head_dim}",
                        "kv_seq_len": "{kv_seq_len}",
                        "m": "{m}",
                        "kv_head_major": true
                    },
                    {
                        "op": "MatMul",
                        "m": "{m}",
                        "n": "{d_model}",
                        "k": "{d_model}",
                        "transpose_b": true,
                        "b": "layer.attn_output_{i}",
                        "b_scales": "layer.attn_output_{i}",
                        "a": "attn_out",
                        "output": "residual_1",
                        "params": {
                            "batch": 1,
                            "weights_per_block": 32
                        },
                        "bias": "zero",
                        "c": "hidden",
                        "alpha": 1.0,
                        "beta": 1.0,
                        "has_bias": 0
                    },
                    {
                        "op": "FusedFfnSwiGluRmsNorm",
                        "input": "residual_1",
                        "gamma": "layer.ffn_norm_{i}",
                        "w_gate": "layer.ffn_gate_{i}",
                        "w_up": "layer.ffn_up_{i}",
                        "b_gate": "layer.ffn_gate_bias_{i}",
                        "b_up": "layer.ffn_up_bias_{i}",
                        "output": "up",
                        "weights_per_block": 32
                    },
                    {
                        "op": "MatMul",
                        "m": "{m}",
                        "n": "{d_model}",
                        "k": "{ff_dim}",
                        "transpose_b": true,
                        "b": "layer.ffn_down_{i}",
                        "b_scales": "layer.ffn_down_{i}",
                        "a": "up",
                        "output": "hidden",
                        "params": {
                            "batch": 1,
                            "weights_per_block": 32
                        },
                        "bias": "layer.ffn_down_bias_{i}",
                        "c": "residual_1",
                        "alpha": 1.0,
                        "beta": 1.0,
                        "has_bias": 1
                    }
                ]
            },
            {
                "op": "RmsNormV2",
                "input": "hidden",
                "output": "final_norm_out",
                "gamma": "final_norm",
                "feature_dim": "{d_model}",
                "total_elements": "{total_elements_hidden}"
            },
            {
                "op": "MatMul",
                "m": "{m}",
                "n": "{vocab_size}",
                "k": "{d_model}",
                "transpose_b": true,
                "b": "output_weight",
                "b_scales": "output_weight",
                "a": "final_norm_out",
                "output": "logits",
                "params": {
                    "batch": 1,
                    "weights_per_block": 32
                },
                "bias": "zero",
                "c": "zero",
                "alpha": 1.0,
                "beta": 0.0,
                "has_bias": 0
            }
        ]
    }
}
