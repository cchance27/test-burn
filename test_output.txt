   Compiling metallic v0.1.0 (/Volumes/2TB/test-burn/crates/metallic)
warning: unused variable: `b_k_saved`
   --> crates/metallic/src/metals/gemv/qkv_step.rs:235:13
    |
235 |         let b_k_saved = b_k.clone();
    |             ^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_b_k_saved`
    |
    = note: `#[warn(unused_variables)]` (part of `#[warn(unused)]`) on by default

warning: unused variable: `b_v_saved`
   --> crates/metallic/src/metals/gemv/qkv_step.rs:236:13
    |
236 |         let b_v_saved = b_v.clone();
    |             ^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_b_v_saved`

warning: `metallic` (lib) generated 2 warnings (run `cargo fix --lib -p metallic` to apply 2 suggestions)
    Finished `test` profile [unoptimized + debuginfo] target(s) in 4.02s
     Running tests/dsl_vs_context_parity.rs (target/debug/deps/dsl_vs_context_parity-0f93dbc15f814727)

running 1 test

=== DSL vs Context Full Forward Parity Test ===

✅ Both models loaded
Prompt: 'Hello' -> tokens: [9707]

--- Running Legacy Forward ---
Legacy embedding shape: [1, 1, 896]
[LEGACY] layer 0 x_normed_attn first 5: [0.04034424, 0.028625488, -0.02468872, 0.14904785, 0.02998352]
[LEGACY] layer 0 v_heads first 5: [-0.009811401, -0.021850586, -0.017440796, -0.0037078857, 0.016784668]
[LEGACY] layer 0 q_heads_after_rope first 5: [-0.030273438, 0.023925781, -0.14746094, -0.103027344, -15.65625]
[LEGACY] layer 0 attn_out first 5: [0.0014781952, 0.004840851, -0.0039749146, -0.011207581, 0.015357971]
[LEGACY] layer 0 hidden norm: 7.1973, first 5: [-0.06048584, 0.29882813, -0.15771484, 0.2064209, 0.023284912]
[LEGACY] layer 1 hidden norm: 11.5879, first 5: [-0.026550293, 0.4345703, -0.1977539, 0.12817383, -0.15771484]
[LEGACY] layer 2 hidden norm: 1008.8809, first 5: [1.6455078, -6.328125, -4.296875, 0.7636719, -4.5195313]
[LEGACY] layer 3 hidden norm: 1617.1390, first 5: [1.1396484, -8.2421875, -4.7148438, 1.3007813, -2.4648438]
[LEGACY] layer 4 hidden norm: 1620.9238, first 5: [1.2138672, -8.0703125, -4.6054688, 1.1757813, -2.2792969]
[LEGACY] layer 5 hidden norm: 1648.4043, first 5: [1.25, -8.1484375, -4.5898438, 1.0087891, -2.171875]
[LEGACY] layer 6 hidden norm: 1649.6451, first 5: [1.2011719, -8.15625, -4.4765625, 0.8544922, -2.2304688]
[LEGACY] layer 7 hidden norm: 1650.8911, first 5: [1.2685547, -8.1015625, -4.5195313, 0.8071289, -2.140625]
[LEGACY] layer 8 hidden norm: 1652.1844, first 5: [1.3085938, -8.1171875, -4.4726563, 0.6425781, -2.0351563]
[LEGACY] layer 9 hidden norm: 1652.5442, first 5: [1.2734375, -8.2578125, -4.4609375, 0.5473633, -1.9873047]
[LEGACY] layer 10 hidden norm: 1653.5394, first 5: [1.2451172, -8.1328125, -4.3984375, 0.35791016, -1.8789063]
[LEGACY] layer 11 hidden norm: 1653.3328, first 5: [1.2675781, -8.25, -4.4726563, 0.22338867, -1.7587891]
[LEGACY] layer 12 hidden norm: 1653.2839, first 5: [1.3222656, -8.34375, -4.4492188, 0.3544922, -1.6347656]
[LEGACY] layer 13 hidden norm: 1652.9458, first 5: [1.2910156, -8.515625, -4.421875, 0.33569336, -1.5322266]
[LEGACY] layer 14 hidden norm: 1652.7583, first 5: [1.3291016, -8.5390625, -4.4804688, 0.28198242, -1.4902344]
[LEGACY] layer 15 hidden norm: 1653.2411, first 5: [1.3945313, -8.3671875, -4.4648438, 0.5239258, -1.5527344]
[LEGACY] layer 16 hidden norm: 1652.9329, first 5: [1.4794922, -8.3125, -4.421875, 0.43408203, -1.5849609]
[LEGACY] layer 17 hidden norm: 1652.1738, first 5: [1.4619141, -8.3515625, -4.6953125, 0.38061523, -1.4853516]
[LEGACY] layer 18 hidden norm: 1650.6282, first 5: [1.6591797, -8.3828125, -4.71875, 0.18188477, -1.75]
[LEGACY] layer 19 hidden norm: 1641.2950, first 5: [1.8339844, -8.6640625, -3.9414063, -0.90771484, -4.8398438]
[LEGACY] layer 20 hidden norm: 1636.3528, first 5: [2.1796875, -8.546875, -3.7792969, -1.3984375, -4.8710938]
[LEGACY] layer 21 hidden norm: 1011.4952, first 5: [0.68652344, -12.046875, -3.296875, 1.8466797, -3.9941406]
[LEGACY] layer 22 hidden norm: 550.0452, first 5: [3.140625, -3.78125, 3.2675781, 8.3046875, 2.0390625]
[LEGACY] layer 23 hidden norm: 502.1641, first 5: [6.7890625, 4.8203125, 0.62890625, 4.7148438, 4.390625]
Legacy hidden shape: [1, 1, 896]
Legacy logits shape: [1, 1, 151936]
Legacy logits first 10: [1.5820313, -1.0019531, -0.6694336, -1.4238281, -0.93310547, 2.4082031, 0.5683594, 1.3925781, 2.6738281, -1.2226563]

--- Running DSL Forward ---
Set globals: seq_len=1, total_elements_hidden=896

--- Step-by-step DSL execution ---
[Step 0] Embedding
  → hidden: max_diff=0.000000, avg_diff=0.00000000, mismatch=None
  ✅ Embedding matches
[Step 1] Repeat
    [Layer 0, SubStep 0] FusedQkv
DEBUG FusedQkv Source:
#include "policies/policy_f16.metal"

kernel void fused_qkv_rmsnorm_f16(
    const device uchar* w_q [[buffer(0)]],
    const device uchar* s_q [[buffer(1)]],
    const device uchar* w_k [[buffer(2)]],
    const device uchar* s_k [[buffer(3)]],
    const device uchar* w_v [[buffer(4)]],
    const device uchar* s_v [[buffer(5)]],
    const device half* input [[buffer(6)]],
    constant uint& k_dim [[buffer(7)]],
    constant uint& n_dim [[buffer(8)]],
    constant uint& n_kv [[buffer(9)]],
    constant uint& weights_per_block [[buffer(10)]],
    device half* out_q [[buffer(11)]],
    device half* out_k [[buffer(12)]],
    device half* out_v [[buffer(13)]],
    const device half* b_q [[buffer(14)]],
    const device half* b_k [[buffer(15)]],
    const device half* b_v [[buffer(16)]],
    constant uint& has_b [[buffer(17)]],
    const device half* gamma [[buffer(18)]],
    uint3 gid [[threadgroup_position_in_grid]],
    uint3 lid [[thread_position_in_threadgroup]],
    uint3 tptg [[threads_per_threadgroup]]
) {
    uint idx = gid.x;

    // Prologue 0
    // Warp-per-row layout indices
    const uint warp_id = lid.x / SIMD_WIDTH;
    const uint lane_id = lid.x & (SIMD_WIDTH - 1u);
    const uint row_idx = gid.x * WARPS_PER_TG + warp_id;  // Output index (N)
    const uint tid = lane_id;
    
    // Early exit if row is out of bounds
    if (row_idx >= n_dim) return;
    
    // Layout: [N, K] Blocked

    // Prologue 1

    // --- RmsNorm Compute Stage ---
    // Compute inv_rms of the input vector for Apply fusion
    threadgroup float tg_inv_rms_storage;
    float inv_rms = rmsnorm_compute_inv_rms<PolicyF16>(
        (const device uchar*)input, 
        (const device uchar*)input, // Dummy scale_bytes (F16 ignores)
        k_dim, // feature_dim
        0,     // row_idx (Input is vector, always row 0)
        lane_id,
        warp_id,
        &tg_inv_rms_storage
    );
        
    // Main computation

    // Parallel QKV Projection (PolicyF16)
    const uint blocks_per_k = (k_dim + weights_per_block - 1) / weights_per_block;
    float acc_q = 0.0f, acc_k = 0.0f, acc_v = 0.0f;
    uint k_base = 0;

    // Use absolute indexing inside the loop to support blocked layouts (Canonical)
    // Scale pointers can still use row offsets as they are simple
    const device uchar* row_s_q = s_q + (ulong)row_idx * blocks_per_k * 2;
    const device uchar* row_s_k = s_k + (ulong)row_idx * blocks_per_k * 2;
    const device uchar* row_s_v = s_v + (ulong)row_idx * blocks_per_k * 2;

    while (k_base + K_CHUNK_SIZE <= k_dim) {
        uint k = k_base + lane_id * 8u;
        float4 xv_raw = *(const device float4*)(input + k);
        half4 xv_lo = as_type<half4>(xv_raw.xy);
        half4 xv_hi = as_type<half4>(xv_raw.zw);

        
        // --- Fused Norm Application ---
        if (gamma) {
             float4 f_lo = float4(xv_lo);
             float4 f_hi = float4(xv_hi);
             f_lo *= inv_rms;
             f_hi *= inv_rms;
             const device half* g_ptr = gamma + k;
             f_lo.x *= (float)g_ptr[0]; f_lo.y *= (float)g_ptr[1]; 
             f_lo.z *= (float)g_ptr[2]; f_lo.w *= (float)g_ptr[3];
             f_hi.x *= (float)g_ptr[4]; f_hi.y *= (float)g_ptr[5]; 
             f_hi.z *= (float)g_ptr[6]; f_hi.w *= (float)g_ptr[7];
             xv_lo = half4(f_lo);
             xv_hi = half4(f_hi);
        }
            
        
        float4 xv_f32_lo = float4(xv_lo);
        float4 xv_f32_hi = float4(xv_hi);
        
        uint block_off = k / weights_per_block;
        // Optimization: Use half scales directly and only load once per block
        // (Compiler will likely optimize these further)
        float s_q_val = (float)PolicyF16::load_scale(row_s_q, block_off);
        float s_k_val = (row_idx < n_kv) ? (float)PolicyF16::load_scale(row_s_k, block_off) : 0.0f;
        float s_v_val = (row_idx < n_kv) ? (float)PolicyF16::load_scale(row_s_v, block_off) : 0.0f;

        // Q Dot
        {
            float w[8];
            const device uchar* w_ptr = w_q + WEIGHT_INDEX(row_idx, k, k_dim, n_dim) * 2;
            PolicyF16::template load_weights<8>(w_ptr, 0, w);
            acc_q += s_q_val * (dot(xv_f32_lo, float4(w[0],w[1],w[2],w[3])) + dot(xv_f32_hi, float4(w[4],w[5],w[6],w[7])));
        }
        
        // K & V Dot
        if (row_idx < n_kv) {
            {
                float w[8];
                const device uchar* w_ptr = w_k + WEIGHT_INDEX(row_idx, k, k_dim, n_kv) * 2;
                PolicyF16::template load_weights<8>(w_ptr, 0, w);
                acc_k += s_k_val * (dot(xv_f32_lo, float4(w[0],w[1],w[2],w[3])) + dot(xv_f32_hi, float4(w[4],w[5],w[6],w[7])));
            }
            {
                float w[8];
                const device uchar* w_ptr = w_v + WEIGHT_INDEX(row_idx, k, k_dim, n_kv) * 2;
                PolicyF16::template load_weights<8>(w_ptr, 0, w);
                acc_v += s_v_val * (dot(xv_f32_lo, float4(w[0],w[1],w[2],w[3])) + dot(xv_f32_hi, float4(w[4],w[5],w[6],w[7])));
            }
        }

        k_base += K_CHUNK_SIZE;
    }
    
    // Tail: bounds-checked
    if (k_base < k_dim) {
        uint k = k_base + lane_id * 8u;
        float4 xv_raw = float4(0.0f);
        uint valid_count = 0;
        
        if (k + 8u <= k_dim) {
            xv_raw = *(const device float4*)(input + k);
            valid_count = 8u;
        } else if (k < k_dim) {
            for (uint i = 0; i < 8u && k + i < k_dim; ++i) {
                ((thread half*)&xv_raw)[i] = input[k + i];
                valid_count++;
            }
        }
        
        half4 xv_lo = as_type<half4>(xv_raw.xy);
        half4 xv_hi = as_type<half4>(xv_raw.zw);

        
        // --- Fused Norm Application ---
        if (gamma) {
             float4 f_lo = float4(xv_lo);
             float4 f_hi = float4(xv_hi);
             f_lo *= inv_rms;
             f_hi *= inv_rms;
             const device half* g_ptr = gamma + k;
             f_lo.x *= (float)g_ptr[0]; f_lo.y *= (float)g_ptr[1]; 
             f_lo.z *= (float)g_ptr[2]; f_lo.w *= (float)g_ptr[3];
             f_hi.x *= (float)g_ptr[4]; f_hi.y *= (float)g_ptr[5]; 
             f_hi.z *= (float)g_ptr[6]; f_hi.w *= (float)g_ptr[7];
             xv_lo = half4(f_lo);
             xv_hi = half4(f_hi);
        }
            
        
        float4 xv_f32_lo = float4(xv_lo);
        float4 xv_f32_hi = float4(xv_hi);
        
        uint block_off = k / weights_per_block;
        float s_q_val = (k < k_dim) ? (float)PolicyF16::load_scale(row_s_q, block_off) : 0.0f;
        float s_k_val = (row_idx < n_kv && k < k_dim) ? (float)PolicyF16::load_scale(row_s_k, block_off) : 0.0f;
        float s_v_val = (row_idx < n_kv && k < k_dim) ? (float)PolicyF16::load_scale(row_s_v, block_off) : 0.0f;

        // Q Dot
        if (k < k_dim) {
            float w[8] = {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f};
            const device uchar* w_ptr = w_q + WEIGHT_INDEX(row_idx, k, k_dim, n_dim) * 2;
            PolicyF16::template load_weights<8>(w_ptr, 0, w);
            for (uint i = valid_count; i < 8u; ++i) w[i] = 0.0f;
            acc_q += s_q_val * (dot(xv_f32_lo, float4(w[0],w[1],w[2],w[3])) + dot(xv_f32_hi, float4(w[4],w[5],w[6],w[7])));
        }
        
        // K & V Dot
        if (row_idx < n_kv && k < k_dim) {
            {
                float w[8] = {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f};
                const device uchar* w_ptr = w_k + WEIGHT_INDEX(row_idx, k, k_dim, n_kv) * 2;
                PolicyF16::template load_weights<8>(w_ptr, 0, w);
                for (uint i = valid_count; i < 8u; ++i) w[i] = 0.0f;
                acc_k += s_k_val * (dot(xv_f32_lo, float4(w[0],w[1],w[2],w[3])) + dot(xv_f32_hi, float4(w[4],w[5],w[6],w[7])));
            }
            {
                float w[8] = {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f};
                const device uchar* w_ptr = w_v + WEIGHT_INDEX(row_idx, k, k_dim, n_kv) * 2;
                PolicyF16::template load_weights<8>(w_ptr, 0, w);
                for (uint i = valid_count; i < 8u; ++i) w[i] = 0.0f;
                acc_v += s_v_val * (dot(xv_f32_lo, float4(w[0],w[1],w[2],w[3])) + dot(xv_f32_hi, float4(w[4],w[5],w[6],w[7])));
            }
        }
    }
    
    float3 qkv_partial = float3(acc_q, acc_k, acc_v);

    // Epilogue 0

    uint mask = 0xFFFFFFFF;
    float3 qkv_sum = qkv_partial;
    for (uint offset = 16; offset > 0; offset /= 2) {
        qkv_sum.x += simd_shuffle_down(qkv_sum.x, offset);
        qkv_sum.y += simd_shuffle_down(qkv_sum.y, offset);
        qkv_sum.z += simd_shuffle_down(qkv_sum.z, offset);
    }
    float3 qkv_final = qkv_sum;

    // Epilogue 1

    if (lane_id == 0) {
        out_q[row_idx] = half(qkv_final.x + (has_b ? (float)b_q[row_idx] : 0.0f));
        if (row_idx < n_kv) {
            out_k[row_idx] = half(qkv_final.y + (has_b ? (float)b_k[row_idx] : 0.0f));
            out_v[row_idx] = half(qkv_final.z + (has_b ? (float)b_v[row_idx] : 0.0f));
        }
    }

}

DEBUG FusedQkv Layer 0: k_dim=896 n_dim=896 n_kv=128 weights_per_block=32 | q_off=0 k_off=0 v_off=0 input_off=0
DEBUG read_f16_buffer: ptr=0x14c38c000 offset=0 size=802816 product=1605632
DEBUG FusedQkv Weights (w_q) sample (first 5): [-0.010421753, -0.02168274, 0.0033893585, 0.005405426, 0.0054130554]
DEBUG FusedQkv Weights (w_q) non-zeros: 802779 / 802816
DEBUG FusedQkv out_q buffer ptr BEFORE run: 0x1106e1700
DEBUG read_f16_buffer: ptr=0x1106d9a00 offset=0 size=896 product=1792
DEBUG FusedQkv Input (hidden) sample (first 5): [-0.009460449, 0.0055236816, 0.0058898926, 0.022705078, -0.0062561035]
DEBUG FusedQkv Input (hidden) non-zeros: 896 / 896
DEBUG read_f16_buffer: ptr=0x1106e1700 offset=0 size=896 product=1792
DEBUG FusedQkv OutQ sample (first 5): [-0.030273438, 0.023925781, -0.14746094, -0.103027344, -15.65625]
DEBUG FusedQkv OutQ index 666: 17.3125
DEBUG read_f16_buffer: ptr=0x108afea00 offset=0 size=896 product=1792
DEBUG FusedQkv BiasQ index 666: 17.125
DEBUG FusedQkv OutQ non-zeros: 896 / 896
      → q first 5: [-0.030273438, 0.023925781, -0.14746094, -0.103027344, -15.65625]
    [Layer 0, SubStep 1] KvRearrange
      → q_heads first 5: [-0.030273438, 0.023925781, -0.14746094, -0.103027344, -15.65625]
    [Layer 0, SubStep 2] KvRearrange
    [Layer 0, SubStep 3] KvRearrange
      → v_heads first 5: [-0.009811401, -0.021850586, -0.017440796, -0.0037078857, 0.016784668]
    [Layer 0, SubStep 4] Rope
      → q_rot first 5: [-0.030273438, 0.023925781, -0.14746094, -0.103027344, -15.65625]
    [Layer 0, SubStep 5] Rope
    [Layer 0, SubStep 6] KvCacheWrite
    [Layer 0, SubStep 7] KvCacheWrite
    [Layer 0, SubStep 8] RepeatKvHeads
    [Layer 0, SubStep 9] RepeatKvHeads
    [Layer 0, SubStep 10] SdpaMaterialized
      → attn_out first 5: [-0.009811401, -0.021850586, -0.017440796, -0.0037078857, 0.016784668]
    [Layer 0, SubStep 11] GemvCanonical
      → residual_1 first 5: [-0.007980347, 0.010360718, 0.001914978, 0.011497498, 0.009101868]
    [Layer 0, SubStep 12] RmsNorm
    [Layer 0, SubStep 13] GemvCanonical
    [Layer 0, SubStep 14] GemvCanonical
    [Layer 0, SubStep 15] Swiglu
    [Layer 0, SubStep 16] GemvCanonical
      → hidden first 5: [-0.06060791, 0.29907227, -0.15771484, 0.20629883, 0.023254395]
[FOUNDRY] layer 0 sync and norm check
[FOUNDRY] layer 0 hidden norm: 7.1962
[FOUNDRY] layer 1 sync and norm check
[FOUNDRY] layer 1 hidden norm: 11.5859
[FOUNDRY] layer 2 sync and norm check
[FOUNDRY] layer 2 hidden norm: 1009.3837
[FOUNDRY] layer 3 sync and norm check
[FOUNDRY] layer 3 hidden norm: 1617.1663
[FOUNDRY] layer 4 sync and norm check
[FOUNDRY] layer 4 hidden norm: 1620.9510
[FOUNDRY] layer 5 sync and norm check
[FOUNDRY] layer 5 hidden norm: 1648.4308
[FOUNDRY] layer 6 sync and norm check
[FOUNDRY] layer 6 hidden norm: 1649.6731
[FOUNDRY] layer 7 sync and norm check
[FOUNDRY] layer 7 hidden norm: 1650.9180
[FOUNDRY] layer 8 sync and norm check
[FOUNDRY] layer 8 hidden norm: 1652.2119
[FOUNDRY] layer 9 sync and norm check
[FOUNDRY] layer 9 hidden norm: 1652.5719
[FOUNDRY] layer 10 sync and norm check
[FOUNDRY] layer 10 hidden norm: 1653.5670
[FOUNDRY] layer 11 sync and norm check
[FOUNDRY] layer 11 hidden norm: 1653.3594
[FOUNDRY] layer 12 sync and norm check
[FOUNDRY] layer 12 hidden norm: 1653.3118
[FOUNDRY] layer 13 sync and norm check
[FOUNDRY] layer 13 hidden norm: 1652.9735
[FOUNDRY] layer 14 sync and norm check
[FOUNDRY] layer 14 hidden norm: 1652.7864
[FOUNDRY] layer 15 sync and norm check
[FOUNDRY] layer 15 hidden norm: 1653.2689
[FOUNDRY] layer 16 sync and norm check
[FOUNDRY] layer 16 hidden norm: 1652.9612
[FOUNDRY] layer 17 sync and norm check
[FOUNDRY] layer 17 hidden norm: 1652.2025
[FOUNDRY] layer 18 sync and norm check
[FOUNDRY] layer 18 hidden norm: 1650.6559
[FOUNDRY] layer 19 sync and norm check
[FOUNDRY] layer 19 hidden norm: 1641.2679
[FOUNDRY] layer 20 sync and norm check
[FOUNDRY] layer 20 hidden norm: 1636.3268
[FOUNDRY] layer 21 sync and norm check
[FOUNDRY] layer 21 hidden norm: 1011.3882
[FOUNDRY] layer 22 sync and norm check
[FOUNDRY] layer 22 hidden norm: 549.6413
[FOUNDRY] layer 23 sync and norm check
[FOUNDRY] layer 23 hidden norm: 501.6873
  → hidden after layers (pre-final norm): max_diff=0.5000, avg_diff=0.007459, mismatch=Some(62)
[Step 2] RmsNorm
  → hidden after final norm: max_diff=0.0625, avg_diff=0.003699, mismatch=None
[Step 3] GemvCanonical

--- DSL step-by-step completed ---
DSL logits shape: [151936]
DSL logits first 10: [1.5830078, -1.0058594, -0.67529297, -1.4238281, -0.9355469, 2.4121094, 0.56640625, 1.3955078, 2.671875, -1.2246094]

--- Comparing Logits ---
Logits comparison: max_diff=0.0195, avg_diff=0.003926, first_mismatch=None
✅ Full forward parity PASSED
test test_dsl_vs_context_full_forward_parity ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 6 filtered out; finished in 39.87s

